# -*- coding: utf-8 -*-
"""2.0_IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14WAoX8OypB4HgLsI0oiUFaYY0pox0ziS
"""

from google.colab import drive
drive.mount('/content/drive')

pip install contractions

import pandas as pd
import os
import numpy as np
import seaborn as sns
import re
import gzip
import json
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WordPunctTokenizer
import unicodedata
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from keras.models import Sequential
import contractions
import re
import inflect
from keras.layers import Dense, Dropout,Embedding, LSTM, GRU
from keras.regularizers import l1_l2
from sklearn.preprocessing import StandardScaler
from keras.preprocessing.sequence import pad_sequences
from sklearn.decomposition import TruncatedSVD

imdb = pd.read_csv('/content/drive/MyDrive/TEXT_MINING/IMDb_Reviews.csv')

imdb.head()

len(imdb)

#count of positive sentiment
(imdb.sentiment == 1).sum()

#count of negative sentiment
(imdb.sentiment == 0).sum()

imdb['review_length'] = imdb['review'].str.len()

plt.figure(figsize=(12.8,6))
sns.distplot(imdb['review_length']).set_title('Distribuzione della lunghezza delle review');

imdb.head()

"""# Preprocessing

### Normalization
"""

#Lower case and contract

def lower_and_contract(review):
  norm_review = contractions.fix(review.lower())
  return norm_review

# Remove newlines \n 

def remove_n(review):
  norm_review = review.replace("\n", "")
  return norm_review

# Remove tab \t 

def remove_t(review):
  norm_review = review.replace("\t", "")
  return norm_review

# Remove br

def remove_br(review):
  norm_review = review.replace("<br>", "").replace('<br />', "").replace('<br/>', "")
  return norm_review

# Remove urls

def remove_urls(review):
  norm_review = re.sub(r'http\S+', '', review)
  return norm_review

#Remove punctuation

def remove_punct(review):
  norm_review = re.sub(r'[^\w\s]', ' ', review)
  return norm_review

# Remove whitespaces at the beginning and at the end of the string

def remove_whitespace(review):
  norm_review = review.strip()
  return norm_review

# Remove digits

def remove_digits(review):
  norm_review = re.sub(r'\d+', '', review)
  return norm_review

# Remove emoji

emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001F1F2-\U0001F1F4"  # Macau flag
        u"\U0001F1E6-\U0001F1FF"  # flags
        u"\U0001F600-\U0001F64F"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U0001F1F2"
        u"\U0001F1F4"
        u"\U0001F620"
        u"\u200d"
        u"\u2640-\u2642"
        "]+", flags=re.UNICODE)

def remove_emoji(review):
  norm_review = emoji_pattern.sub(r'', review)
  return norm_review

# Remove extraspaces 
def remove_spaces(review):
  norm_review = re.sub("\s\s+", " ", review)
  return norm_review

# Function with all normalization phases
def normalization(review):
    review = lower_and_contract(review)
    review = remove_digits(review)
    review = remove_whitespace(review)
    review = remove_urls(review)
    review = remove_n(review)
    review = remove_t(review)
    review = remove_br(review)
    review = remove_emoji(review)
    review = remove_punct(review)
    review = remove_spaces(review)

    return review

norm_imdb = []
for elem in imdb['review']:
    norm_imdb.append(normalization(elem))

imdb['norm_review'] = norm_imdb

imdb.head()

imdb.norm_review[10]

"""### Stop words removal"""

stop_words = set(stopwords.words('english')) 

def remove_stopwords(review):
    """Remove stop words"""
    new_review = [w for w in review.split() if not w in stop_words]
    long_words=[]
    for i in new_review:
        if len(i)>=1:                  
            long_words.append(i)   

    return (" ".join(long_words)).strip()

new_imdb = []
for elem in imdb['norm_review']:
    new_imdb.append(remove_stopwords(elem))

imdb['no_stop_words'] = new_imdb

imdb.head()

"""###Lemmatization and tokenization"""

lemmatizer = WordNetLemmatizer()
tokenizer = WordPunctTokenizer()

def lemmatize(text):
  lemma_review = [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)]
  return lemma_review

lemma_imdb = []
for elem in imdb['no_stop_words']:
    lemma_imdb.append(lemmatize(elem))

imdb['lemma_review'] = lemma_imdb
imdb.head()

imdb['lemma_length'] = imdb['lemma_review'].str.len()

plt.figure(figsize=(12.8,6))
sns.distplot(imdb['lemma_length']).set_title('Distribuzione della lunghezza delle review dopo il preprocessing');

"""# TEXT REPRESENTATION"""

def token_to_phrase(token_list):
    phrase_list = []
    
    for phrase in token_list:
        seq = ""
        for word in phrase:
            seq = seq + " " + word 
        phrase_list.append(seq)    
    return phrase_list

lemmed_preprocessed  = token_to_phrase(token_list = imdb['lemma_review'])

imdb["lemmed_preprocessed"] = token_to_phrase(token_list = imdb['lemma_review'])
imdb.head()

"""SENTIMENTAL ANALYSIS

VADER
"""

import nltk
nltk.download('opinion_lexicon') #A list of positive and negative opinion words or sentiment words for English.
nltk.download('vader_lexicon') #VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned
#to sentiments expressed in social media, and works well on texts from other domains.

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
imdb_sentimental = pd.read_csv('/content/drive/MyDrive/TEXT_MINING/IMDb_Reviews.csv')

def sentiment_score(x):
    sentence = x['lemmed_preprocessed']
    ss = sid.polarity_scores(sentence)
    return ss

imdb['Sentiment_Score_Vader'] = imdb.apply(lambda x: sentiment_score(x), axis=1)

imdb = [imdb, pd.DataFrame(imdb['Sentiment_Score_Vader'].tolist()).iloc[:, :5]]
df2 = pd.concat(imdb, axis=1).drop('Sentiment_Score_Vader', axis=1)

def min_max_scaling(x):
  s1 = x[['compound']]
  z = (s1 - (-1)) / (1 - (-1))
  return z

#(valore-min) / (massimo-min)
df2['Normalize_Compound'] = df2.apply(lambda x: min_max_scaling(x), axis=1)

pd.set_option('display.max_columns', 25)
df2

#def prova(x):
    sentimentoo = []
    x = x[['Normalize_Compound']]
    for k in sorted(x):
      if (k > 0.5):
          sentimentoo = 1
      elif (k <= 0.5):
          sentimentoo = 0
    return sentimentoo

df2['Vader_Riasssuntivo'] = df2['pos'] - df2['neg']
df2

def prova2(x):
    sentimentoo = []
    x = x[['Vader_Riasssuntivo']]
    for k in sorted(x):
      if (k < 0):
          sentimentoo = 0
      elif (k >= 0):
          sentimentoo = 1
    return sentimentoo

df2['Label_Vader'] = df2.apply(lambda x: prova2(x), axis=1)

df2

df = df2[["review","Label_Vader"]]

result = pd.concat([df, imdb_sentimental], axis=1)

result1 = result[["review","Label_Vader","sentiment"]]
result2 = result1.drop(columns = ['review'])
result2

result1

pip install afinn

from textblob import TextBlob

def sentiment_calc(text):
    try:
        return TextBlob(text).sentiment
    except:
        return None

df2['sentiment_blob'] = df2['lemmed_preprocessed'].apply(sentiment_calc)

df2

from afinn import Afinn

afinn = Afinn() 
def sentiment_calc2(text):
    try:
        return afinn.score(text)
    except:
        return None

df2['sentiment_afinn'] = df2['lemmed_preprocessed'].apply(sentiment_calc2)
df2

dfblob = df2
dfblob = [dfblob, pd.DataFrame(dfblob['sentiment_blob'].tolist()).iloc[:, :17]]
df5 = pd.concat(dfblob, axis=1).drop('sentiment_blob', axis=1)
df5

def min_max_scaling2(x):
  s1 = x[['polarity']]
  z = (s1 - (-1)) / (1 - (-1))
  return z
df5['norm'] = df5.apply(lambda x: min_max_scaling2(x), axis=1)
def label(x):
    sentimentoo = []
    x = x[['norm']]
    for k in sorted(x):
      if (k < 0.5):
          sentimentoo = 0
      elif (k >= 0.5):
          sentimentoo = 1
    return sentimentoo

result1['Label_Blob'] = df5.apply(lambda x: label(x), axis=1)
result1

#column = df5["sentiment_afinn"] 
#max_value = column.max()
#min_value = column.min()
#max_value, min_value 
#(183.0, -137.0)
def min_max_scaling3(x):
  s1 = x[['sentiment_afinn']]
  z = (s1 - (-137)) / (183 - (-137))
  return z
df5['norm_afinn'] = df5.apply(lambda x: min_max_scaling3(x), axis=1)
def lablel2(x):
    sentimentoo = []
    x = x[['norm_afinn']]
    for k in sorted(x):
      if (k < 0.5):
          sentimentoo = 0
      elif (k >= 0.5):
          sentimentoo = 1
    return sentimentoo

result1['Label_Afinn'] = df5.apply(lambda x: lablel2(x), axis=1)

result2 = result1.drop(columns = ['review'])
comparison_column = np.where(result2["Label_Vader"] == result2["sentiment"], True, False)
result2["Vader"] = comparison_column
comparison_column2 = np.where(result2["Label_Afinn"] == result2["sentiment"], True, False)
result2["Afinn"] = comparison_column2
comparison_column3 = np.where(result2["Label_Blob"] == result2["sentiment"], True, False)
result2["Blob"] = comparison_column3
Matrix_Afini = pd.DataFrame({'Count': result2.groupby(['Afinn']).size()}) 
Matrix_Vader = pd.DataFrame({'Count': result2.groupby(['Vader']).size()}) 
Matrix_Blob = pd.DataFrame({'Count': result2.groupby(['Blob']).size()})

Matrix_Afini, Matrix_Vader, Matrix_Blob